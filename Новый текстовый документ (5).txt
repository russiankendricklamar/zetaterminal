#@title Инструкция
"""
# # #
# Об интерфейсе
# # #

        Осуществляет поиск информации в RU DATA по заданным параметрам, и выдаёт готовый результат.


        def api_extractAndLoad(self,
                               path_method:str,
                               body_json:dict,
                               post:bool=True,
                               search_array:Iterable=None,
                               search_param:str='filter',
                               search_field:str=None,
                               print_mode:str='normal',
                               save_to_drive:bool=False) -> pd.DataFrame:


        :param str path_method:         Метод интерфейса API RU DATA, путь расположения искомых данных.

        :param dict body_json:          Запрос в формате JSON.

        :param bool post:               (опционально) Использовать метод POST в качестве HTTP-запроса. В противном случае будет использоваться метод GET.

        :param Iterable search_array:   (опционально) Массив поиска. Следует указывать только в том случае, если массив поиска имеет больше 100 записей.
                                                      В обратном случае можно занести массив поиска в сам запрос - в параметры, способные принять массив в качестве агрумента.

        :param str search_param:        (опционально) Параметр тела запроса, который принимает массив поиска в качестве аргумента.
                                                      Указывается вместе с массивом поиска.

        :param str search_field:        (опционально) Имя поля в адресуемом методе API RU DATA, по которому проводится фильтрация с массивом поиска.
                                                    Указывается вместе с массивом поиска и только если search_param == 'filter'.

        :param str print_mode:          (опционально) Режим печати о результатах выгрузки в консоль. Имеются 3 режима:
                                                    - normal: Печатать основную информацию о выгрузке.
                                                    - silent: Печатать в консоль только об ошибках.
                                                    - debug:  Печатать всю возможную информацию о выгрузке.

        :param bool save_to_drive:      (опционально) Сохранить результаты выгрузки в файл .csv на Google Drive.
                                                      При этом не возвращается датафрейм в целях высвобождения оперативной памяти.
                                                      Выгрузка производится в папку ../RU DATA API/ВЫГРУЗКИ/
                                                      Рекомендуется включать при выгрузке большого объема данных.

        :return: pandas.DataFrame
                NoneType - в случае ошибки


# # #
# Составление запросов
# # #

Пути и параметры запросов к данным расположены здесь:
https://docs.efir-net.ru/dh2/#/

Путь к нужной информации передаётся функции api_extractAndLoad строкой как аргумент path_method.
Запрос к данным передаётся посредством JSON (dict) как аргумент body_json.
    - Слева в запросе указываются параметры поиска, справа их значения.
      И то, и другое можно найти на указанном сайте в разделе "Свойства" для каждого метода.
    - Все параметры, кроме 'filter', 'ids' и ему подобных, задаются на ваше усмотрение.
      Параметры 'pageNum' и 'pageSize' задавать не нужно.

Пример корректного пути и запроса:

    path = 'Indicator/Constituents'
    body = {
        'filter': "fintoolid IN ('RU00000xxxx1', 'RU00000xxxx2', 'XS00000xxxx3') and UPDATE_DATE > #2022-08-30#",
        'dateFrom': '2022-12-01',
        'dateTo': '2023-03-31'
    }
    df = api_extractAndLoad(path, body)

В случае выгрузок с большими массивам поиска (больше 100), в api_extractAndLoad нужно подставить доп. аргументы:
    - search_array (массив поиска - итерируемый объект);
    - search_field (имя поля в адресуемой БД, к которой применяется массив поиска).
    - search_param (название JSON параметра, в значение которого записывается массив поиска.
                    У каждого метода он может быть свой, но чаще всего это 'filter', поэтому он поставлен по-умолчанию.
                    Прописан в каждом методе RU DATA как "Строка фильтрации").

Пример корректного пути и запроса (в случае с большим массивом поиска):

    path = 'Indicator/List'
    body = {}
    df = api_extractAndLoad(path,
                            body,
                            search_array=tuple(df_test['id']),
                            search_field='fintoolid',
                            search_param='filter')


# # #
# Примечания
# # #

- Массивы поиска принимаются только в круглых скобках. Пример: "isin IN ('XS2399474761', 'XS2432286974')".

- Составление даты и времени.
  Дата:         #YYYY-mm-dd#
  Дата-время:   #YYYY-mm-ddThh:MM:SS#

- Вся доп. информация, инструкции и примеры запросов расположены здесь:
  https://docs.efir-net.ru/dh2/#/Instructions/query_params

Спасибо за внимание ☻
"""

#@title Выгрузчик RU DATA
from sys import getsizeof
import tracemalloc
import os
from os.path import exists
from datetime import date, datetime, timedelta
import asyncio
from asyncio.exceptions import CancelledError
from typing import Iterable, Optional, Union

import aiohttp
from tqdm.notebook import tqdm
import pandas as pd

from google.colab import drive
import nest_asyncio
nest_asyncio.apply()

class RudataExtractor:
    def __init__(self,
                 max_pagesize=100,
                 max_array=100,
                 multi=True,
                 login='cbrsar-web-05',
                 password='454Qwx'):
        # Запрос данных из RUDATA.
        # ВАЖНО! Ограничения:
        #     1. Необходимо отправлять не более 5 запросов в секунду.
        #     2. pageSize должен быть не более 300 (макс. 300 объектов за запрос) - однако некоторые методы требуют 100, поэтому для обратной совместимости используется лимит 100.
        #     3. В фильтре к запросу и в массиве поиска должно быть не более 100 элементов за раз.
        self.__API_URL = 'https://dh2.efir-net.ru/v2/'
        self.__UPLOAD_PATH = "/content/drive/My Drive/RU DATA API/ВЫГРУЗКИ/"
        self.__CONST_MAX_REQUESTS = 5
        self.__CONST_REQUEST_WAIT = 1.0 if multi else 0.2
        self.__CONST_TOKEN_REFRESH = 1800.0
        try:
            assert isinstance(max_pagesize, int) and max_pagesize > -1
            self.__API_PAGESIZE_MAX = max_pagesize

            assert isinstance(max_array, int) and max_array > -1
            self.__API_SEARCHARRAY_MAX = max_array

            self.__API_REQUEST_MAX = self.__CONST_MAX_REQUESTS if multi else 1

            assert isinstance(login, str)
            self.__API_LOGIN = login

            assert isinstance(password, str)
            self.__API_PASS = password

        except AssertionError:
            raise ValueError('<class.RudataExtractor>: Incorrect value for one of the arguments')

        print(f"""
▌\t\tВыгрузчик RU DATA\t\t
▌\tСтартовые параметры:\t\t\t
▌\t- Размер страницы выборки: {self.__API_PAGESIZE_MAX}\t\t
▌\t- Размер поискового массива: {self.__API_SEARCHARRAY_MAX}\t
▌\t- Многопотоковая выгрузка: {'Включена' if multi else 'Выключена'}\t
▌\t- Имя авторизуемого аккаунта: {self.__API_LOGIN}\t
        """)

    #
    # Вспомогательные методы.
    #
    def info(self):
        print(f"""
▌\tТекущие параметры:\t\t\t
▌\t- Размер страницы выборки: {self.__API_PAGESIZE_MAX}\t\t
▌\t- Размер поискового массива: {self.__API_SEARCHARRAY_MAX}\t
▌\t- Многопотоковая выгрузка: {'Включена' if self.__API_REQUEST_MAX == self.__CONST_MAX_REQUESTS else 'Выключена'}\t
▌\t- Имя авторизуемого аккаунта: {self.__API_LOGIN}\t
        """)


    def info_reset(self,
                   silent=False):
        self.__API_PAGESIZE_MAX = 100
        self.__API_SEARCHARRAY_MAX = 100
        self.__API_REQUEST_MAX = self.__CONST_MAX_REQUESTS

        if not silent:
            self.info()


    def info_set(self,
                 max_pagesize=None,
                 max_array=None,
                 multi=None,
                 login=None,
                 password=None,
                 silent=False):
        try:
            if max_pagesize is None:
                pass
            elif self.__API_PAGESIZE_MAX != max_pagesize:
                assert isinstance(max_pagesize, int) and max_pagesize > -1
                self.__API_PAGESIZE_MAX = max_pagesize

            if max_array is None:
                pass
            elif self.__API_SEARCHARRAY_MAX != max_array:
                assert isinstance(max_array, int) and max_array > -1
                self.__API_SEARCHARRAY_MAX = max_array

            if multi is None:
                pass
            else:
                self.__API_REQUEST_MAX = self.__CONST_MAX_REQUESTS if multi else 1

            if login is None:
                pass
            elif self.__API_LOGIN != login:
                assert isinstance(login, str)
                self.__API_LOGIN = login

            if password is None:
                pass
            elif self.__API_PASS != password:
                assert isinstance(password, str)
                self.__API_PASS = password
        except AssertionError:
            raise ValueError('<method.info_set>: Incorrect value for one of the arguments')

        if not silent:
            self.info()


    # Загрузка вручную обработанного датафрейма на Drive.
    # #
    def fit_into_drive(self,
                       df,
                       path_method,
                       rewrite=False):
        if not isinstance(df, pd.DataFrame):
            raise ValueError('<method.fit_into_drive>: Argument is not a dataframe instance')
        drive.mount('/content/drive')
        upload_path = self.__UPLOAD_PATH
        if not exists(upload_path + "Incremental/"):
            os.mkdir(upload_path + "Incremental/")
        upload_path += "Incremental/" + date.today().strftime('%Y-%m/')
        if not exists(upload_path):
            os.mkdir(upload_path)
        filename = path_method.replace('/','_') + ".csv"
        df.to_csv(upload_path + filename,
                  sep=';', decimal=',', date_format='%Y-%m-%dT%H:%M:%S', index=False, encoding='cp1251',
                  mode='w' if (rewrite or not exists(upload_path + filename)) else 'a',
                  header=True if (rewrite or not exists(upload_path + filename)) else False)


    def __gen_window(self, sequence, n):
        j = 0
        while j + n < len(sequence):
            yield tuple(sequence[j : j+n])
            j += n
        if len(sequence[j : j+n]) <= n:
            yield tuple(sequence[j : j+n])


    async def __timer_callback(self):
        try:
            await asyncio.sleep(self.__CONST_REQUEST_WAIT)
        except CancelledError: raise


    async def __inspect_token(self):
        try:
            await asyncio.sleep(self.__CONST_TOKEN_REFRESH)
        except CancelledError: raise

    # #
    # # Служебные методы выгрузки из RU DATA.
    # #
    async def __doRequest(self, session, url, body, token, mode='POST'):
        try:
            if (token is None):
                headers = {'Content-Type': 'application/json'}
            else:
                headers = {'authorization': 'Bearer ' + token, 'Content-Type': 'application/json'}

            if mode in ('POST', 'GET'):
                async with session.request(mode, url, json=body, headers=headers) as response:
                    if response.ok:
                        return await response.json()
                    return " ".join([str(response.status), response.reason])
            else:
                raise ValueError('<method.doRequest>: Incorrect value for argument \'mode\': ' + mode)
        except CancelledError: raise


    async def __getToken(self, session, login, password):
        try:
            url = self.__API_URL + 'Account/Login'
            body = {
                'login': login,
                'password': password
            }
            token = await self.__doRequest(session, url, body, None)
            if isinstance(token, int):
                print(f'<method.getToken>: Token error (code {token})',
                    'Ошибка авторизации. Проверьте логин и пароль.', sep='\n')
                return None

            return token['token']
        except CancelledError: raise

    # # #
    # # # Основной метод выгрузки из RU DATA.
    # # #
    async def __extractAndLoad(self,
                               path_method: str,
                               body_json: dict,
                               post: Optional[bool],
                               search_array: Optional[Iterable],
                               search_field: Optional[str],
                               search_param: Optional[str],
                               print_mode: Optional[str],
                               keep_duplicates: Optional[bool],
                               save_to_drive: Optional[Union[bool, str]],
                               safe: Optional[bool]) -> pd.DataFrame | None:

        session = aiohttp.ClientSession()
        if print_mode == 'debug': tracemalloc.start()

        try:
            token = await self.__getToken(session, self.__API_LOGIN, self.__API_PASS)
            if token is None:
                return None
            token_expired = asyncio.ensure_future(self.__inspect_token())

            if path_method.startswith('/'):
                path_method = path_method[1:]
            if path_method.endswith('/'):
                path_method = path_method[:-1]
            if path_method.startswith('v2/'):
                path_method = path_method[3:]

            if print_mode != 'silent':
                print(f'Выгрузка данных RU DATA из {path_method}...')

            if save_to_drive:
                print('Выбран режим выгрузки на диск - данные не сохранятся в оперативной памяти.')
                print('Режим записи:', 'Инкрементальный' if save_to_drive == 'incremental' else 'Обычный')
                drive.mount('/content/drive')
                upload_path = self.__UPLOAD_PATH
                if save_to_drive == 'incremental':
                    if not exists(upload_path + "Incremental/"):
                        os.mkdir(upload_path + "Incremental/")
                    if not exists(upload_path + "Incremental/" + date.today().strftime('%Y-%m/')):
                        os.mkdir(upload_path + "Incremental/" + date.today().strftime('%Y-%m/'))
                    upload_path += "Incremental/" + date.today().strftime('%Y-%m/')
                    filename = path_method.replace('/','_') + ".csv"
                else:
                    filename = path_method.replace('/','_') + '_' + (datetime.today() + timedelta(hours=3)).strftime('%Y-%m-%d_%H-%M-%S') + ".csv"

            info_destination = []
            prev_container = None

            is_first_pass = True

            # 1. Выгрузка по параметрам поиска.
            if search_array is None:
                body_json['pageNum'] = 1
                body_json['pageSize'] = self.__API_PAGESIZE_MAX
                body_json['pager'] = {'page': 1, 'size': self.__API_PAGESIZE_MAX}

                while True:
                    if save_to_drive and getsizeof(info_destination) > 10_000_000:
                        df = pd.DataFrame({key: tuple(entry.get(key) for entry in info_destination) for key in info_destination[0].keys()})
                        df = df.replace([r'\r', r'\n'], [' ', ' '], regex=True) \
                               .to_csv(upload_path + filename,
                                       sep=';', decimal=',', date_format='%Y-%m-%dT%H:%M:%S', index=False, encoding='cp1251',
                                       mode='w' if ((is_first_pass and save_to_drive != 'incremental') or (save_to_drive == 'incremental' and not exists(upload_path + filename))) else 'a',
                                       header=True if ((is_first_pass and save_to_drive != 'incremental') or (save_to_drive == 'incremental' and not exists(upload_path + filename))) else False)
                        del df
                        info_destination = []
                        is_first_pass = False

                    if token_expired.done():
                        token = await self.__getToken(session, self.__API_LOGIN, self.__API_PASS)
                        token_expired = asyncio.ensure_future(self.__inspect_token())

                    tasks_stack = []
                    for _ in range(self.__API_REQUEST_MAX):
                        tasks_stack.append(asyncio.create_task(self.__doRequest(session,
                                                                                self.__API_URL + path_method,
                                                                                body_json,
                                                                                token,
                                                                                mode='POST' if post else 'GET')))
                        await asyncio.sleep(0)
                        body_json['pageNum'] += 1
                        body_json['pager']['page'] += 1
                    timer_permission = asyncio.ensure_future(self.__timer_callback())

                    done = await asyncio.gather(*tasks_stack, return_exceptions=False)
                    for parse_container in done:
                        if print_mode == 'debug':
                            try:
                                print('\r', f'Memory/Container (MB): {tracemalloc.get_traced_memory()[0]/1e6} / {getsizeof(info_destination)/1e6}', '|',
                                      'Duplicate:', parse_container == prev_container, f"(page {body_json['pageNum']})", '|',
                                      type(parse_container), len(parse_container), '|',
                                      parse_container[0], end='')
                            except:
                                print('\r', f'Memory/Container (MB): {tracemalloc.get_traced_memory()[0]/1e6} / {getsizeof(info_destination)/1e6}', '|',
                                      type(parse_container), parse_container, end='')

                        if isinstance(parse_container, str):
                            if safe:
                                print('<Function.doRequest>: Bad request: ' + parse_container,
                                      'Ошибка: данные не найдены. Запрос мог быть неправильно составлен.', sep='\n')
                                if print_mode == 'debug':
                                    print('<Query body>:', body_json, sep='\n')
                                return None
                            else:
                                break

                        if body_json['pageNum'] <= self.__API_REQUEST_MAX + 2:
                            if parse_container == prev_container:
                                if print_mode != 'silent':
                                    print('Обнаружена цикличная выгрузка одинаковых массивов данных.',
                                          'Скорее всего, в текущем методе API отсутствует постраничная выгрузка. Проверьте запрос на корректность.', sep='\n')
                                break
                            prev_container = parse_container

                        if isinstance(parse_container, dict):
                            info_destination += [parse_container]
                            break
                        if len(parse_container) < self.__API_PAGESIZE_MAX:
                            info_destination += parse_container
                            break

                        info_destination += parse_container

                    else:
                        await timer_permission
                        if not timer_permission.cancelled():
                            timer_permission.cancel()
                        else:
                            timer_permission = None
                        continue
                    break

            # 2. Выгрузка по массиву поиска.
            else:
                if search_param is None:
                    raise TypeError("1 required positional argument: 'search_param'")
                if search_param == 'filter' and search_field is None:
                    raise TypeError("1 required positional argument: 'search_field'")
                if search_param in body_json.keys() and body_json[search_param]:
                    filter_orig = body_json[search_param]
                else:
                    filter_orig = None

                duplicate_checked = False

                for window in tqdm(self.__gen_window(search_array, self.__API_SEARCHARRAY_MAX),
                                   total=len(search_array) // self.__API_SEARCHARRAY_MAX + bool(len(search_array) % self.__API_SEARCHARRAY_MAX),
                                   ncols=500, mininterval=3.0, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{rate_fmt}{postfix}]', colour='green',
                                   disable=print_mode == 'silent'):
                    if search_param == 'filter':
                        if filter_orig is not None and search_param in body_json.keys() and body_json[search_param]:
                            body_json[search_param] = filter_orig + f" AND {search_field} IN {str(window).replace(',', '') if len(window) == 1 else window}"
                        else:
                            body_json[search_param] = f"{search_field} IN {str(window).replace(',', '') if len(window) == 1 else window}"
                    else:
                        body_json[search_param] = list(window)

                    body_json['pageNum'] = 1
                    body_json['pageSize'] = self.__API_PAGESIZE_MAX
                    body_json['pager'] = {'page': 1, 'size': self.__API_PAGESIZE_MAX}

                    if save_to_drive and getsizeof(info_destination) > 10_000_000:
                        df = pd.DataFrame({key: tuple(entry.get(key) for entry in info_destination) for key in info_destination[0].keys()})
                        df = df.replace([r'\r', r'\n'], [' ', ' '], regex=True) \
                               .to_csv(upload_path + filename,
                                     sep=';', decimal=',', date_format='%Y-%m-%dT%H:%M:%S', index=False, encoding='cp1251',
                                     mode='w' if ((is_first_pass and save_to_drive != 'incremental') or (save_to_drive == 'incremental' and not exists(upload_path + filename))) else 'a',
                                     header=True if ((is_first_pass and save_to_drive != 'incremental') or (save_to_drive == 'incremental' and not exists(upload_path + filename))) else False)
                        del df
                        info_destination = []
                        is_first_pass = False

                    while True:
                        if token_expired.done():
                            token = await self.__getToken(session, self.__API_LOGIN, self.__API_PASS)
                            token_expired = asyncio.ensure_future(self.__inspect_token())

                        tasks_stack = []
                        for _ in range(self.__API_REQUEST_MAX):
                            tasks_stack.append(asyncio.create_task(self.__doRequest(session,
                                                                                    self.__API_URL + path_method,
                                                                                    body_json,
                                                                                    token,
                                                                                    mode='POST' if post else 'GET')))
                            await asyncio.sleep(0)
                            body_json['pageNum'] += 1
                            body_json['pager']['page'] += 1
                        timer_permission = asyncio.ensure_future(self.__timer_callback())

                        done = await asyncio.gather(*tasks_stack, return_exceptions=False)
                        for parse_container in done:
                            if print_mode == 'debug':
                                try:
                                    print('\r', f'Memory/Container (MB): {tracemalloc.get_traced_memory()[0]/1e6} / {getsizeof(info_destination)/1e6}', '|',
                                          'Duplicate:', parse_container == prev_container, f"(page {body_json['pageNum']})", '|',
                                          type(parse_container), len(parse_container), '|',
                                          parse_container[0], end='')
                                except:
                                    print('\r', f'Memory/Container (MB): {tracemalloc.get_traced_memory()[0]/1e6} / {getsizeof(info_destination)/1e6}', '|',
                                          type(parse_container), parse_container, end='')

                            if isinstance(parse_container, str):
                                if safe:
                                    print('<Function.doRequest>: Bad request: ' + parse_container,
                                          'Ошибка: данные не найдены. Запрос мог быть неправильно составлен.', sep='\n')
                                    if print_mode == 'debug':
                                        print('<Query body>:', body_json, sep='\n')
                                    return None
                                else:
                                    break

                            if body_json['pageNum'] <= self.__API_REQUEST_MAX + 2:
                                if parse_container == prev_container:
                                    if print_mode != 'silent' and not duplicate_checked:
                                        print('Обнаружена цикличная выгрузка одинаковых массивов данных.',
                                              'Скорее всего, в текущем методе API отсутствует постраничная выгрузка. Проверьте запрос на корректность.', sep='\n')
                                        duplicate_checked = True
                                    break
                                prev_container = parse_container

                            if isinstance(parse_container, dict):
                                info_destination += [parse_container]
                                break
                            if len(parse_container) < self.__API_PAGESIZE_MAX:
                                info_destination += parse_container
                                break

                            info_destination += parse_container

                        else:
                            await timer_permission
                            if not timer_permission.cancelled():
                                timer_permission.cancel()
                            else:
                                timer_permission = None
                            continue
                        break
                if print_mode != 'silent':
                    print(end='\n')
        except CancelledError:
            raise
        finally:
            await session.close()
            if print_mode == 'debug': tracemalloc.stop()

        if save_to_drive:
            df = pd.DataFrame()
            try:
                df = pd.DataFrame({key: tuple(entry.get(key) for entry in info_destination) for key in info_destination[0].keys()})
            finally:
                df = df.replace([r'\r', r'\n'], [' ', ' '], regex=True) \
                       .to_csv(upload_path + filename,
                               sep=';', decimal=',', date_format='%Y-%m-%dT%H:%M:%S', index=False, encoding='cp1251',
                               mode='w' if ((is_first_pass and save_to_drive != 'incremental') or (save_to_drive == 'incremental' and not exists(upload_path + filename))) else 'a',
                               header=True if ((is_first_pass and save_to_drive != 'incremental') or (save_to_drive == 'incremental' and not exists(upload_path + filename))) else False)
                if print_mode != 'silent':
                    print(f'Данные из {path_method} успешно выгружены в файл \'{filename}\'\n')
                del df
                return None
        else:
            if len(info_destination) != 0:
                if print_mode != 'silent':
                    print(f'Данные из {path_method} получены успешно!')
                    print('Кол-во записей:', len(info_destination), '\n')
                if keep_duplicates:
                    return pd.DataFrame({key: tuple(entry.get(key) for entry in info_destination) for key in info_destination[0].keys()}) \
                                .replace([r'\r', r'\n'], [' ', ' '], regex=True) \
                                .reset_index(drop=True)
                else:
                    try:
                        return pd.DataFrame({key: tuple(entry.get(key) for entry in info_destination) for key in info_destination[0].keys()}) \
                                    .drop_duplicates() \
                                    .replace([r'\r', r'\n'], [' ', ' '], regex=True) \
                                    .reset_index(drop=True)
                    except TypeError:
                        return pd.DataFrame({key: tuple(entry.get(key) for entry in info_destination) for key in info_destination[0].keys()}) \
                                    .replace([r'\r', r'\n'], [' ', ' '], regex=True) \
                                    .reset_index(drop=True)
            elif print_mode != 'silent':
                print(f'Данных из {path_method} по указанному запросу не найдено.\n')
            return pd.DataFrame()

    #
    # Метод выгрузки из RU DATA - интерфейс.
    #
    def api_extractAndLoad(self,
                           path_method: str,
                           body_json: dict,
                           post: Optional[bool] = True,
                           search_array: Optional[Iterable] = None,
                           search_param: Optional[str] = 'filter',
                           search_field: Optional[str] = None,
                           print_mode: Optional[str] = 'normal',
                           keep_duplicates: Optional[bool] = False,
                           save_to_drive: Optional[Union[bool, str]] = False,
                           safe: Optional[bool] = True) -> pd.DataFrame | None:
        """ Осуществляет поиск информации в RU DATA по заданным параметрам, и выдаёт готовый результат.

        :param str path_method:         Метод интерфейса API RU DATA, путь расположения искомых данных.

        :param dict body_json:          Запрос в формате JSON.

        :param bool post:               (опционально) Использовать метод POST в качестве HTTP-запроса. В противном случае будет использоваться метод GET.

        :param Iterable search_array:   (опционально) Массив поиска. Следует указывать только в том случае, если массив поиска имеет больше 100 записей.
                                                    В обратном случае можно занести массив поиска в сам запрос - в параметры, способные принять массив в качестве агрумента.

        :param str search_param:        (опционально) Параметр тела запроса, который принимает массив поиска в качестве аргумента.
                                                    Указывается вместе с массивом поиска.

        :param str search_field:        (опционально) Имя поля в адресуемом методе API RU DATA, по которому проводится фильтрация с массивом поиска.
                                                    Указывается вместе с массивом поиска и только если search_param == 'filter'.

        :param str print_mode:          (опционально) Режим печати о результатах выгрузки в консоль. Имеются 3 режима:
                                                    - normal: Печатать основную информацию о выгрузке.
                                                    - silent: Печатать в консоль только об ошибках.
                                                    - debug:  Печатать всю возможную информацию о выгрузке.

        :param bool save_to_drive:      (опционально) Сохранить результаты выгрузки в файл .csv на Google Drive.
                                                      При этом датафрейм не возвращается в целях высвобождения оперативной памяти.
                                                      Выгрузка производится в папку ../RU DATA API/ВЫГРУЗКИ/
                                                      Рекомендуется включать при выгрузке большого объема данных.
                                                      Имеются 2 режима:
                                                    - False / True: Обычный режим записи в папку ../RU DATA API/ВЫГРУЗКИ/
                                                    - 'incremental': Режим записи данных поверх существующих. В файл дозаписываются новые строки в течение текущего месяца, потом создается новый.

        :return: pandas.DataFrame
                 NoneType - в случае ошибки
        """
        try:
            df = asyncio.run(self.__extractAndLoad(path_method, body_json, post, search_array, search_field, search_param, print_mode, keep_duplicates, save_to_drive, safe))
        finally:
            for task in asyncio.all_tasks():
                task.cancel()
        return df

#@title Пример использования

# # 1. Активируем выгрузчик, при желании задаем ему пользовательские параметры (можно не задавать).
extr = RudataExtractor(max_pagesize=100, max_array=100, multi=True)

# # 2. Задаем путь к данным через Контроллер/Метод.
path = '/v2/Bond/CalculateBondMulti'

# # 3. Строим запрос согласно описанию метода в документации RU DATA.
body = {}

# # 4. Запускаем выгрузку через метод, подставив в аргументы путь и запрос.
df = extr.api_extractAndLoad(path, body)
df

extr = RudataExtractor(max_pagesize=100, max_array=100, multi=True)
extr.api_extractAndLoad('v2/Info/FintoolReferenceData', {'fields': ['amortisedmty'], 'id': 'USP37341AA50'})